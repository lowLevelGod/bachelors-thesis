\chapter{Concluzii}

În această lucrare am ilustrat una din multitudinea de aplicaţii în industrie a 
detecţiei anomaliilor, anume identificarea tranzacţiilor frauduloase. Am explorat un 
set de date adnotat cu informaţii despre tranzacţii bancare şi am arătat cum putem evalua
performanţa unor metode de învăţare automată nesupervizată folosind metrici din domeniul 
învăţării supervizate, mai specific, clasificarea binară.

Deşi tehnicile folosite aparţin învăţarii nesupervizate, am arătat cum putem exploata
cunoştinţele din alte două categorii, anume semi-supervizată şi supervizată. 

Prima a fost 
utilă pentru faptul că am antrenat algoritmi pe un set de date mare fără să utilizăm 
etichetele punctelor, dar apoi am folosit un set de date cu o mărime relativă mult mai mică
pentru a analiza performanţa algoritmilor. 

A doua a fost de folos pentru că ne-a oferit 
o multitudine de metrici care într-un context strict nesupervizat nu ar fi existat, fiind 
nevoie de inspecţia unui om pentru analiză.

Alegerea metricilor corespunzătoare
este probabil una din cele mai dificile părţi atunci când căutăm o soluţie eficientă pentru 
problema dată, mai ales când proporţia de clase este neechilibrată şi acordăm o importanţă mai 
mare unei clase faţă de cealaltă, întrucât o alegere greşită ne poate induce în eroare cu privinţă
la utilitatea reală a algoritmului, după cum am şi demonstrat.

De asemenea, am analizat trei algoritmi cu caracteristici diferite, dar cu 
scopuri similare, anume idenfiticarea anomaliilor.

One Class SVM se 
foloseşte de un hiperplan de separare pentru a împărţi spaţiul Hilbert într-o porţiune 
care cuprinde toate punctele normale şi una care cuprinde restul punctelor izolate. Astfel,
putem spune că acesta face o clasificare dură a anomaliilor, întrucât ne indică doar daca
o observaţie este normală sau nu. 

La polul opus, avem Gaussian Mixture Model şi Kernel Density 
Estimation care încearcă să estimeze funcţia densitate de probabilitate din care au fost 
generate datele şi să atribuie fiecărei observaţii noi o valoare dată de această funcţie.
Peste aceste probabilităţi se aplică un prag şi abia apoi obţinem etichetele corespunzătoare.
Astfel, putem spune că acestea fac o clasificare slabă a anomaliilor, întrucât ne indică 
cât de probabil este ca punctul să fie normal sau nu, dar nu ne indică concret clasa 
de care aparţine.

Deşi am întâmpinat probleme în ilustrarea graficului ROC Curve pentru algoritmul cu clasificare
dură, am demonstrat cum putem transforma distanţele produse de funcţia de decizie în valori 
de încredere pentru a obţine o clasificare slabă artificială.

La final, am observat că modelele au performat mai mult sau mai puţin la fel, cele mai complexe 
având un dezavantaj când vine vorba de complexitatea de timp.

Cu toate acestea, lucrarea nu a abordat problema seriilor de timp, lucru des întâlnit în practică,
dar care creează o direcţie de cercetare interesantă. Două cerinţe relevante pentru acest 
subiect ar fi prognoza şi clasificarea seriilor de timp. 

Prima se referă la prezicerea evoluţiei
în timp a seriei de date şi este folosită ca suport pentru unele modele ce detectează anomaliile
folosindu-se de deviaţia prognozei faţă de valorile observate. Prognoza se bazează strict pe 
valorile punctelor din şir din contextul precedent şi pe modelul antrenat anterior. Deseori,
se utilizează o fereastră glisantă de mărime unitară pentru a crea contextul şi a prezice 
câte un punct pe rând. Este similar cu abordarea clasică 
din domeniul Procesării Limbajului Natural, unde suntem interesaţi să prezicem cuvântul cel 
mai potrivit pentru continuarea unei propoziţii, spre exemplu. Aici, putem privi cuvintele
din text ca fiind puncte dintr-o serie de timp.

A doua se referă la atribuirea unei categorii pentru diverse subşiruri din seriile de timp.
Astfel, metoda poate fi folosită ori pentru post-procesarea rezultatelor obţinute după 
detecţia anomaliilor pentru a categoriza diferitele valori deviante, ori chiar în cadrul 
modelului cu rolul de a clasifica componentele din seriile de date în grupuri relevante,
implict anomalii sau observaţii normale. Metoda ne aduce aminte de clasica problemă
a clasificării unor puncte dintr-un set de date, întâlnită în contextul învăţării automate
supervizate \cite{time-series}.


[REMOVE LATER] APPENDIX 

Contamination pentru isolation forest este ales in functie 
de procentul de anomalii pe care il avem in setul de date.

\begin{table}[H]
    \caption{Isolation forest cu contamination 0.017 cel mai bun f1 score 0.1799}
    \centering
    \begin{tabularx}{\textwidth}{
        |X
        |X
        |X
        |X
        |X
        |X|
    }
    \hline
    $trees$ & $sub_samples$ & {Accuracy} & {Recall} & {Precision} & {F1 Score} \\
    \hline
    1 & 32 & 0.5409 & 0.0946 & 0.0127 & 0.0224 \\
    1 & 128 & 0.6546 & 0.3243 & 0.0357 & 0.0643 \\
    1 & 256 & 0.5009 & 0.0135 & 0.0020 & 0.0035 \\
    1 & 512 & 0.9049 & 0.8243 & 0.0898 & 0.1620 \\
    1 & 1024 & 0.7910 & 0.5946 & 0.0756 & 0.1341 \\
    1 & 2048 & 0.8925 & 0.7973 & 0.1014 & 0.1799 \\
    1 & 4096 & 0.6980 & 0.4054 & 0.0693 & 0.1183 \\
    1 & 8192 & 0.8720 & 0.7568 & 0.0932 & 0.1659 \\
    1 & 16384 & 0.7555 & 0.5270 & 0.0542 & 0.0982 \\
    1 & 32768 & 0.7844 & 0.5811 & 0.0761 & 0.1346 \\
    10 & 16 & 0.5591 & 0.1351 & 0.0137 & 0.0249 \\
    10 & 32 & 0.8704 & 0.7568 & 0.0759 & 0.1379 \\
    10 & 64 & 0.8022 & 0.6216 & 0.0591 & 0.1079 \\
    10 & 128 & 0.8780 & 0.7703 & 0.0861 & 0.1549 \\
    10 & 256 & 0.8368 & 0.6892 & 0.0708 & 0.1285 \\
    10 & 512 & 0.8436 & 0.7027 & 0.0730 & 0.1321 \\
    10 & 1024 & 0.8704 & 0.7568 & 0.0762 & 0.1384 \\
    10 & 2048 & 0.7829 & 0.5811 & 0.0619 & 0.1118 \\
    10 & 4096 & 0.8771 & 0.7703 & 0.0764 & 0.1390 \\
    10 & 8192 & 0.8706 & 0.7568 & 0.0779 & 0.1412 \\
    10 & 16384 & 0.8908 & 0.7973 & 0.0808 & 0.1468 \\
    10 & 32768 & 0.8705 & 0.7568 & 0.0771 & 0.1400 \\
    100 & 4 & 0.7425 & 0.5000 & 0.0548 & 0.0988 \\
    100 & 8 & 0.8299 & 0.6757 & 0.0685 & 0.1244 \\
    100 & 16 & 0.8497 & 0.7162 & 0.0690 & 0.1259 \\
    100 & 32 & 0.8839 & 0.7838 & 0.0786 & 0.1429 \\
    100 & 64 & 0.8164 & 0.6486 & 0.0665 & 0.1206 \\
    100 & 128 & 0.8569 & 0.7297 & 0.0735 & 0.1335 \\
    100 & 256 & 0.8503 & 0.7162 & 0.0733 & 0.1330 \\
    100 & 512 & 0.8502 & 0.7162 & 0.0730 & 0.1325 \\
    100 & 1024 & 0.8773 & 0.7703 & 0.0788 & 0.1430 \\
    100 & 2048 & 0.8639 & 0.7432 & 0.0768 & 0.1392 \\
    100 & 4096 & 0.8638 & 0.7432 & 0.0761 & 0.1380 \\
    100 & 8192 & 0.8708 & 0.7568 & 0.0793 & 0.1436 \\
    100 & 16384 & 0.8976 & 0.8108 & 0.0829 & 0.1504 \\
    100 & 32768 & 0.8977 & 0.8108 & 0.0833 & 0.1511 \\
    1000 & 4 & 0.7353 & 0.4865 & 0.0502 & 0.0910 \\
    1000 & 8 & 0.8367 & 0.6892 & 0.0702 & 0.1275 \\
    1000 & 16 & 0.8234 & 0.6622 & 0.0692 & 0.1253 \\
    1000 & 32 & 0.8367 & 0.6892 & 0.0702 & 0.1275 \\
    1000 & 64 & 0.8437 & 0.7027 & 0.0740 & 0.1338 \\
    1000 & 128 & 0.8502 & 0.7162 & 0.0725 & 0.1317 \\
    1000 & 256 & 0.8570 & 0.7297 & 0.0748 & 0.1357 \\
    1000 & 512 & 0.8570 & 0.7297 & 0.0749 & 0.1358 \\
    1000 & 1024 & 0.8638 & 0.7432 & 0.0764 & 0.1385 \\
    1000 & 2048 & 0.8638 & 0.7432 & 0.0765 & 0.1387 \\
    1000 & 4096 & 0.8841 & 0.7838 & 0.0800 & 0.1452 \\
    1000 & 8192 & 0.8909 & 0.7973 & 0.0822 & 0.1490 \\
    1000 & 16384 & 0.8909 & 0.7973 & 0.0821 & 0.1488 \\
    1000 & 32768 & 0.8977 & 0.8108 & 0.0833 & 0.1511 \\
    10000 & 4 & 0.8297 & 0.6757 & 0.0675 & 0.1227 \\
    10000 & 8 & 0.8568 & 0.7297 & 0.0732 & 0.1330 \\
    10000 & 16 & 0.8434 & 0.7027 & 0.0715 & 0.1298 \\
    10000 & 32 & 0.8437 & 0.7027 & 0.0738 & 0.1335 \\
    10000 & 64 & 0.8435 & 0.7027 & 0.0724 & 0.1313 \\
    10000 & 128 & 0.8502 & 0.7162 & 0.0730 & 0.1325 \\
    10000 & 256 & 0.8570 & 0.7297 & 0.0745 & 0.1352 \\
    10000 & 512 & 0.8569 & 0.7297 & 0.0740 & 0.1343 \\
    10000 & 1024 & 0.8637 & 0.7432 & 0.0754 & 0.1370 \\
    10000 & 2048 & 0.8706 & 0.7568 & 0.0777 & 0.1409 \\
    10000 & 4096 & 0.8842 & 0.7838 & 0.0808 & 0.1465 \\
    10000 & 8192 & 0.8909 & 0.7973 & 0.0825 & 0.1496 \\
    10000 & 16384 & 0.8909 & 0.7973 & 0.0822 & 0.1490 \\
    10000 & 32768 & 0.8909 & 0.7973 & 0.0818 & 0.1484 \\ \hline
    \end{tabularx}
\end{table}


\begin{table}[H]
    \caption{KDE cu contamination 0 cel mai bun f1 score 0.5596}
    \centering
    \begin{tabularx}{\textwidth}{
        |X
        |X
        |X
        |X
        |X
        |X|
    }
    \hline
    {Bandwidth} & {Quantile} & {Accuracy} & {Recall} & {Precision} & {F1 Score} \\
    \hline
    0.5 & 0.0001 & 0.5019 & 0.0041 & 0.25 & 0.008 \\
    0.5 & 0.001 & 0.5527 & 0.1057 & 0.7222 & 0.1844 \\
    0.5 & 0.01 & 0.7631 & 0.5325 & 0.3659 & 0.4338 \\
    0.5 & 0.1 & 0.9163 & 0.9268 & 0.0637 & 0.1192 \\
    1.0 & 0.0001 & 0.5081 & 0.0163 & 1.0 & 0.032 \\
    1.0 & 0.001 & 0.5568 & 0.1138 & 0.7778 & 0.1986 \\
    1.0 & 0.01 & 0.8408 & 0.6870 & 0.4721 & 0.5596 \\
    1.0 & 0.1 & 0.9101 & 0.9146 & 0.0629 & 0.1176 \\
    5.0 & 0.0001 & 0.5040 & 0.0081 & 0.5 & 0.016 \\
    5.0 & 0.001 & 0.5507 & 0.1016 & 0.6944 & 0.1773 \\
    5.0 & 0.01 & 0.7774 & 0.5610 & 0.3855 & 0.4570 \\
    5.0 & 0.1 & 0.9142 & 0.9228 & 0.0634 & 0.1187 \\
    5.5 & 0.0001 & 0.5040 & 0.0081 & 0.5 & 0.016 \\
    5.5 & 0.001 & 0.5507 & 0.1016 & 0.6944 & 0.1773 \\
    5.5 & 0.01 & 0.7753 & 0.5569 & 0.3827 & 0.4536 \\
    5.5 & 0.1 & 0.9142 & 0.9228 & 0.0634 & 0.1187 \\
    6.0 & 0.0001 & 0.5040 & 0.0081 & 0.5 & 0.016 \\
    6.0 & 0.001 & 0.5486 & 0.0976 & 0.6667 & 0.1702 \\
    6.0 & 0.01 & 0.7733 & 0.5528 & 0.3799 & 0.4503 \\
    6.0 & 0.1 & 0.9142 & 0.9228 & 0.0634 & 0.1187 \\

    \end{tabularx}
\end{table}

\begin{table}[H]
    \caption{OCSVM kernel rbf, contamination 0, cel mai bun f1 score 0.6611}
    \centering
    \begin{tabularx}{\textwidth}{
        |X
        |X
        |X
        |X
        |X
        |X|
    }
    \hline
    {Gamma} & {Nu} & {Accuracy} & {Recall} & {Precision} & {F1 Score} \\
    \hline
    0.001 & 0.0001 & 0.5772 & 0.1545 & 0.8636 & 0.2621 \\
    0.001 & 0.001 & 0.6052 & 0.2114 & 0.5778 & 0.3095 \\
    0.001 & 0.01 & 0.8914 & 0.7927 & 0.3558 & 0.4912 \\
    0.01 & 0.0001 & 0.8221 & 0.6463 & 0.6766 & 0.6611 \\
    0.01 & 0.001 & 0.8221 & 0.6463 & 0.6766 & 0.6611 \\
    0.01 & 0.01 & 0.9260 & 0.8618 & 0.3786 & 0.5261 \\
    0.02 & 0.0001 & 0.8777 & 0.7602 & 0.5268 & 0.6223 \\
    0.02 & 0.001 & 0.8777 & 0.7602 & 0.5238 & 0.6202 \\
    0.02 & 0.01 & 0.9278 & 0.8659 & 0.3704 & 0.5189 \\
    \hline
    \end{tabularx}
\end{table}


\begin{table}[H]
    \caption{GMM cu contamination 0 cel mai bun f1 score 0.6954}
    \centering
    \begin{tabularx}{\textwidth}{
        |X
        |X
        |X
        |X
        |X
        |X|
    }
    \hline
    {Components} & {Quantile} & {Accuracy} & {Recall} & {Precision} & {F1 Score} \\
    \hline
    1 & 0.0001 & 0.5081 & 0.0163 & 1.0 & 0.032 \\
    1 & 0.001 & 0.5732 & 0.1463 & 1.0 & 0.2553 \\
    1 & 0.01 & 0.9145 & 0.8333 & 0.5726 & 0.6788 \\
    1 & 0.1 & 0.9122 & 0.9187 & 0.0631 & 0.1182 \\
    2 & 0.0001 & 0.5081 & 0.0163 & 1.0 & 0.032 \\
    2 & 0.001 & 0.5732 & 0.1463 & 1.0 & 0.2553 \\
    2 & 0.01 & 0.9145 & 0.8333 & 0.5726 & 0.6788 \\
    2 & 0.1 & 0.9020 & 0.8984 & 0.0617 & 0.1156 \\
    3 & 0.0001 & 0.5081 & 0.0163 & 1.0 & 0.032 \\
    3 & 0.001 & 0.5732 & 0.1463 & 1.0 & 0.2553 \\
    3 & 0.01 & 0.8981 & 0.8008 & 0.5503 & 0.6523 \\
    3 & 0.1 & 0.9142 & 0.9228 & 0.0634 & 0.1187 \\
    4 & 0.0001 & 0.5081 & 0.0163 & 1.0 & 0.032 \\
    4 & 0.001 & 0.5732 & 0.1463 & 1.0 & 0.2553 \\
    4 & 0.01 & 0.8920 & 0.7886 & 0.5419 & 0.6424 \\
    4 & 0.1 & 0.9122 & 0.9187 & 0.0631 & 0.1182 \\
    5 & 0.0001 & 0.5061 & 0.0122 & 0.75 & 0.024 \\
    5 & 0.001 & 0.5691 & 0.1382 & 0.9444 & 0.2411 \\
    5 & 0.01 & 0.8961 & 0.7967 & 0.5475 & 0.6490 \\
    5 & 0.1 & 0.9122 & 0.9187 & 0.0631 & 0.1182 \\
    6 & 0.0001 & 0.5081 & 0.0163 & 1.0 & 0.032 \\
    6 & 0.001 & 0.5711 & 0.1423 & 0.9722 & 0.2482 \\
    6 & 0.01 & 0.9247 & 0.8537 & 0.5866 & 0.6954 \\
    6 & 0.1 & 0.9020 & 0.8984 & 0.0617 & 0.1156 \\
    7 & 0.0001 & 0.5081 & 0.0163 & 1.0 & 0.032 \\
    7 & 0.001 & 0.5732 & 0.1463 & 1.0 & 0.2553 \\
    7 & 0.01 & 0.8736 & 0.7520 & 0.5168 & 0.6126 \\
    7 & 0.1 & 0.8999 & 0.8943 & 0.0615 & 0.1150 \\
    \hline
    \end{tabularx}
\end{table}
