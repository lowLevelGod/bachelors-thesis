\chapter{Preliminarii}

\section{One Class SVM}

\subsection{Ideea algoritmului}

Această metodă este inspirată din clasificatorul cu vectori suport. Ideea este 
sa găsim \textbf{un hiperplan cu margine maximă}, posibil într-un spaţiu cu 
mai multe dimensiuni decât cel iniţial, în funcţie de kernel,
care să separe originea (se presupune că punctele sunt centrate) spaţiului de trăsături
de restul punctelor din setul de date\cite{Schölkopf et al}.

Un alt mod echivalent de a privi algoritmul este găsirea celei mai \textbf{mici 
hipersfere} care să includă toate punctele din setul de date\cite{Tax et al.}.

\subsection{Formularea matematică}

Prima formulare ce implică un hiperplan de separare

\begin{equation}
    \begin{aligned}
    & \underset{w, \rho, \xi}{\text{min}}
    & & \frac{1}{2} \|w\|^2 + \frac{1}{\nu n} \sum_{i=1}^{n} \xi_i - \rho \\
    & \text{cu constrângerea}
    & & \langle w, \phi(x_i) \rangle \geq \rho - \xi_i, \quad i=1,2,\ldots,n \\
    &&& \xi_i \geq 0, \quad i=1,2,\ldots,n \\
    \end{aligned}
    \end{equation}
    
    \begin{itemize}
        \item $w$ este vectorul de pondere al hiperplanului
        \item $\rho$ este termenul de influenţă
        \item $\xi_i$ sunt variabilele de relaxare pentru încălcarea marginii
        \item $\phi(x_i)$ este funcţia de scufundare pentru $x_i$.
        \item $n$ este numărul total de puncte
        \item $\nu$ este marginea superioară pentru ponderea de anomalii şi marginea 
        inferioară pentru ponderea de vectori suport
    
    \end{itemize}

şi forma duală

    \begin{equation}
        \begin{aligned}
        & \underset{\alpha}{\text{min}}
        & & \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j K(x_i, x_j) \\
        & \text{cu constrângerea}
        & & 0 \leq \alpha_i \leq \frac{1}{\nu n}, \quad i=1,2,\ldots,n \\
        &&& \sum_{i=1}^{n} \alpha_i = 1
        \end{aligned}
        \end{equation}
    
    \begin{itemize}
        \item $\alpha_i$ sunt variabilele duale asociate punctelor 
        \item $K(x_i, x_j)$ este funcţia kernel
    \end{itemize}

A doua formulare ce implică găsirea hipersferei minime

    \begin{equation}
        \begin{aligned}
        & \underset{R, \rho, \xi}{\text{min}}
        & & R^2 + \frac{1}{\nu n} \sum_{i=1}^{n} \xi_i \\
        & \text{cu constrângerea}
        & & \|\phi(x_i) - c\|^2 \leq R^2 + \xi_i, \quad i=1,2,\ldots,n \\
        &&& \xi_i \geq 0, \quad i=1,2,\ldots,n \\
        \end{aligned}
        \end{equation}
        
        \begin{itemize}
        \item $R$ este raza hipersferei
        \item $c$ este centrul hipersferei
        \end{itemize}
        

\section{Gaussian Mixture Model}

\subsection{Ideea algoritmului}

Algoritmul încearcă să estimeze funcţia densitate de probabilitate 
din care au fost generate datele folosind 
\textbf{o sumă ponderată de distribuţii Gaussiene}.
Astfel, putem modela distribuţii multimodale utilizând o distribuţie bine cunoscută.
Parametrii necesari sunt ponderile, mediile şi covarianţele fiecărei componente.

\subsection{Formularea matematică}

Funcţia densitate de probabilitate estimată este dată de 

\begin{equation}
    p(x | \theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)
    \end{equation}
    
    \begin{itemize}
        \item $K$ este numărul de componente Gaussiene
        \item $\pi_k$ este ponderea celei de a $k$-a componentă cu $\sum_{k=1}^{K} \pi_k = 1$
        \item $\mathcal{N}(x | \mu_k, \Sigma_k)$ este distribuţia Gaussiană
        cu medie $\mu_k$ şi matrice de covarianţă $\Sigma_k$
    
    Parametrii $\theta$ sunt de obicei învăţaţi din setul de date folosind 
    tehnici precum algoritmul Expectation-Maximization (EM).
    \end{itemize}

\section{Kernel Density Estimation}

\subsection{Ideea algoritmului}

Precum Gaussian Mixture Model, algoritmul încearcă să estimeze 
funcţia densitate de probabilitate din care au fost generate datele.

Un parametru numit \textbf{"lăţime de bandă"} influenţează netezimea distribuţiei 
rezultate.
Cel mai des utilizat kernel este cel Gaussian şi pe acesta îl vom folosi şi noi.

Aici, pentru fiecare punct generăm o distribuţie Gaussiană cu \textbf{media} egală
cu punctul respectiv şi \textbf{deviaţie} egală cu "lăţimea de bandă". Apoi, adunăm toate 
distribuţiile obţinute mai sus si le împărţim la numărul total de puncte.

\subsection{Formularea matematică}

Densitatea estimată de kernel într-un punct $x$ este dată de

\begin{equation}
\hat{f}_h(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)
\end{equation}

\begin{itemize}
    \item $n$ este numărul total de puncte
    \item $h$ este lăţimea de bandă
    \item $K(u)$ este funcţia kernel 
\end{itemize}

\section{Metrici de performanţă}

\subsection{Accuracy}

\begin{equation}
    \text{Accuracy} = \frac{\text{observaţii clasificate corect}}{\text{total observaţii}}
\end{equation}

Această metrică ne indică câte 
\textbf{clasificări făcute de model au fost corecte din 
totalul de puncte care trebuie clasificate}. În cazul nostru, nu este o măsură bună 
a performanţei, întrucât atribuie o importanţă egală anomaliilor şi observaţiilor 
normale. Nedetectarea unei anomalii este mult mai costisitoare.

Totuşi, vom include şi această metrică pentru comparaţie, pe lângă cele relevante.

\subsection{Precision}

\begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
\end{equation}

Precision ne indică \textbf{capacitatea modelului de a nu produce fals pozitive}, în cazul nostru,
de a nu raporta o valoare normală ca fiind anomalie.

\subsection{Recall}

\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

Recall ne indică 
\textbf{capacitatea modelului de a identifica toate observaţiile pozitive},
în cazul nostru, de a detecta toate anomaliile. Având în vedere scopul lucrării,
aceasta va fi o metrică importantă.

\subsection{ROC Curve}

ROC Curve ne ajută să evaluăm calitatea modelului prin reprezentarea grafică 
a ratei de fals pozitiv pe axa X şi a ratei de adevărat pozitiv pe axa Y. 
\textbf{Punctul 
ideal al graficului se află în colţul din stânga sus} pentru ca ne dorim o 
rată de fals pozitiv egală cu 0 şi o rată de adevărat pozitiv egală cu 1. Prin urmare,
ne dorim sa maximizăm rata de adevărat pozitiv şi de a minimiza rata de fals pozitiv.
